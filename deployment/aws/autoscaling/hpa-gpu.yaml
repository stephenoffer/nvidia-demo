# Horizontal Pod Autoscaler for GPU workloads on AWS EKS
# Uses custom metrics for GPU utilization
# Requires: Metrics Server, Prometheus Adapter (for custom metrics), NVIDIA GPU metrics exporter

apiVersion: v1
kind: Namespace
metadata:
  name: pipeline-production
  labels:
    name: pipeline-production
---
# Horizontal Pod Autoscaler with resource and custom metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multimodal-pipeline-hpa
  namespace: pipeline-production
  labels:
    app: multimodal-pipeline
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: multimodal-pipeline
  minReplicas: 3
  maxReplicas: 20
  metrics:
  # CPU utilization metric
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Memory utilization metric
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # GPU utilization metric (requires Prometheus Adapter)
  - type: Pods
    pods:
      metric:
        name: gpu_utilization_percent
      target:
        type: AverageValue
        averageValue: "70"
  # Custom metric: request rate
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  # Custom metric: queue depth
  - type: Pods
    pods:
      metric:
        name: queue_depth
      target:
        type: AverageValue
        averageValue: "50"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 30
      selectPolicy: Max
---
# Vertical Pod Autoscaler (optional, requires VPA operator)
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: multimodal-pipeline-vpa
  namespace: pipeline-production
  labels:
    app: multimodal-pipeline
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: multimodal-pipeline
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: pipeline
      minAllowed:
        cpu: "4"
        memory: 8Gi
      maxAllowed:
        cpu: "64"
        memory: 128Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits


